
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Chetan Srinidhi, ECE, NITK, National Institute of Technology Karnataka, IIIT Hyderabad, IIITH"> 
<meta name="description" content="Chetan Srinidhi's home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Chetan Srinidhi, NITK</title>
</head>
<body>
<!-- Initial Photo and Contact Details ***************** -->
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1>Chetan L Srinidhi</h1><h1>
                </h1></div>

                <h3>Research Associate</h3>
                <p>
                    Centre for Visual Information Technology (CVIT), <br>
                    International Institute of Information Technology, Hyderabad (IIIT-H),<br>
                    Telangana, India - 500032.<br>
                    <br>
                </p>
                <p> 
                    <strong>Personal:&nbsp;</strong>&nbsp;[<a href="https://drive.google.com/open?id=1ipEFTS7pzF8int1E96Lx0nryRp7rSgjY">Curriculum Vitae</a>] and [<a href="https://scholar.google.co.in/citations?user=H6634DcAAAAJ&hl=en" target="_blank">Google Scholar</a>] and [<a href="https://github.com/srinidhiPY" target="_blank">Github</a>]<br /><strong>Email:&nbsp;</strong><em>srinidhipy</em>&nbsp;[at] <em>gmail</em> [dot] <em>com</em></font>
                </p>
            </td>
            <td>
                <img src="./images/Chetan2.jpeg" border="0" width="200"><br>
            </td>
        </tr><tr>
    </tr></tbody>
</table>

<!-- Short Bio and Research Interests ***************** -->
<h2>About Me</h2>
<p align="justify">
I am currently a Research Associate in the <a href="http://cvit.iiit.ac.in/">Centre for Visual Information Technology (CVIT)</a> group at the <a href="https://www.iiit.ac.in/">International Institute of Information Technology, Hyderabad (IIIT-H)</a>, under the supervision of <a href="https://www.iiit.ac.in/people/faculty/jsivaswamy/">Prof. Jayanthi Sivaswamy</a>. 
Previously, I completed my PhD (awaiting for final defense!) at the <a>Department of Electronics and Communication Engineering</a>, <a href="https://www.nitk.ac.in/">National Institute of Technology Karnataka (NITK)</a>, Surathkal, India, under the supervision of <a href="http://ece.nitk.ac.in/faculty/aparna-p">Dr. Aparna P</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=7YrGeNoAAAAJ&view_op=list_works&sortby=pubdate">Dr. Jeny Rajan</a>.</p>
<p align="justify">
My thesis was focused on <b>Pattern Recognition and Machine Learning Framework for Automated Analysis of Retinal Images.</b> I was supported by the MHRD fellowship (Govt. of India) during my PhD. Before joining NITK for my PhD, I obtained my Master's degree from R. V. College of Engineering, Bangalore and Bachelor's degree from Visvesvaraya Technological University, Belgaum, India.  
</p>

<h2>Research Interests</h2>
<p align="justify">
<b>Medical Image Analysis</b>, <b>Computer Vision</b>, <b>Deep Learning</b>, <b>Opthalmic Image Analysis.</b></p>

<p align="justify">
  I am dedicated to bringing modern machine learning methods (including deep learning) to biomedical image analysis for improving anatomical structure segmentation, lesion detection and quantification, anatomical landmark identification and computer-aided diagnosis.
  Specifically, I am interested in developing machine learning/deep learning methods for various 2D/3D medical image analysis involving various topics such as: image segmentation, unsupervised representation learning, visual attention modelling, semantic image segmentation, multi-modal learning and domain adaptation. 
  Currently, I am investigating self-supervised and generative adversarial learning related topics in DL for medical scenarios. 
</p>

<!-- Publications ***************** -->
<h2>Publications</h2>
<table style="width:100%">   
    <tr>
        <td width="500">
        <img src="images/Overall_pipeline.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>              
        <td align="justify"><b>Chetan L Srinidhi</b>, Priyadarshi Rath, Jayanthi Sivaswamy. <b>"Automatic Quantification of Arteriovenous Nicking from Retinal Images Using a Vessel Keypoint Descriptor."</b> <em>To appear in arXiV, 2019.</em>
        <p></p>
        <!--<p>[<a href="" target="_blank">paper</a>]-->
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]--> </p>
        <div style="height:150px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong><center><u>ABSTRACT</u></center></strong>
	In this paper, we present a complete system for detecting AV nicking. The entire system is solely based on analysis of vessel morphology, employing a rich descriptor called the vessel keypoint descriptor (VKD) and requires no knowledge of artery/vein class label for the vessel segments. 
	Vessel junctions and crossover points are identified first using the VKD. Next, the VKD of the crossover points are further analysed to detect and quantify AV nicking into 3 different severity levels. 
	Two solutions, with one being supervised and another unsupervised, are proposed for the AV nicking classification task. 
	The proposed system has been tested on a publicly available dataset of 90 patches with crossover locations and classifying a patch as normal or mild or severe AV nicking case was found to be superior to state-of-the-art methods. 
	The proposed system achieved an area under the receiver operating characteristic curve of 1, sensitivity/specificity of 1/0.92 and the classification accuracy of 1.
	The proposed method is shown to achieve superior performance compared to other AV nicking quantification methods - that strongly depends on artery-vein classification or intensity based features for quantifying AV nicking.  
	This automated method of AV nicking quantification may be employed to assist ophthalmologists in more efficient screening of systemic and neuro-degenerative diseases and in development of improved automated image analysis for clinical diagnosis.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr>
  	    
    <tr>
        <td width="500">
        <img src="images/TIP_A-V.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Automated Method for Retinal Artery/Vein Separation via Graph Search Metaheuristic Approach."</b> <em>IEEE Transactions on Image Processing <b>(TIP)</b>, (in press), 2019.</em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598955" target="_blank">Paper</a>]
        [<a href="https://ieeexplore.ieee.org/ielx7/83/4358840/8598955/Supp_Material_TIP2889534.pdf?tp=&arnumber=8598955" target="_blank">Supplementary Material</a>]    
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]--> [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:50px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong><center><u>ABSTRACT</u></center></strong>
		Separation of the vascular tree into arteries and veins is a fundamental prerequisite in the automatic diagnosis of retinal biomarkers associated with systemic and neurodegenerative diseases. 
		In this paper, we present a novel graph search metaheuristic approach for automatic separation of arteries/veins (A/V) from color fundus images. 
		Our method exploits local information to disentangle the complex vascular tree into multiple subtrees, and global information to label these vessel subtrees into arteries and veins. 
		Given a binary vessel map, a graph representation of the vascular network is constructed representing the topological and spatial connectivity of the vascular structures. 
		Based on the anatomical uniqueness at vessel crossing and branching points, the vascular tree is split into multiple subtrees containing arteries and veins. 
		Finally, the identified vessel subtrees are labeled with A/V based on a set of handcrafted features trained with random forest classifier. 
		The proposed method has been tested on four different publicly available retinal datasets with an average accuracy of 94.7%, 93.2%, 96.8% and 90.2% across AV-DRIVE, CT-DRIVE. INSPIRE-AVR and WIDE datasets, respectively. 
		These results demonstrate the superiority of our proposed approach in outperforming state-of-the- art methods for A/V separation.			
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>

	    
     <tr>
        <td width="500">
        <img src="images/BSPC.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"A Visual Attention Guided Unsupervised Feature Learning for Robust Vessel Delineation in Retinal Images."</b> <em>Biomedical Signal Processing and Control, 2018.</em>
        <p></p>
        <p>[<a href="https://www.sciencedirect.com/science/article/pii/S1746809418301010" target="_blank">Paper</a>]
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]-->  [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong><center><u>ABSTRACT</u></center></strong>
		We propose a novel visual attention guided unsupervised feature learning (VA-UFL) approach to automatically learn the most discriminative features for segmenting vessels in retinal images. 
		Our VA-UFL approach captures both the knowledge of visual attention mechanism and multi-scale contextual information to selectively visualize the most relevant part of the structure in a given local patch. 
		This allows us to encode a rich hierarchical information into unsupervised filtering learning to generate a set of most discriminative features that aid in the accurate segmentation of vessels, even in the presence of cluttered background.
		Our proposed method is validated on the five publicly available retinal datasets: DRIVE, STARE, CHASE_DB1, IOSTAR and RC-SLO. 
		The experimental results show that the proposed approach significantly outperformed the state-of-the-art methods in terms of sensitivity, accuracy and area under the receiver operating characteristic curve across all five datasets. 
		Specifically, the method achieved an average sensitivity greater than 0.82, which is 7% higher compared to all existing approaches validated on DRIVE, CHASE_DB1, IOSTAR and RC-SLO datasets, and outperformed even second-human observer. 
		The method is shown to be robust to segmentation of thin vessels, strong central vessel reflex, complex crossover structures and fares well on abnormal cases.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>  

	    
    <tr>
        <td width="500">
        <img src="images/ISBI17.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, Priyadarshi Rath, Jayanthi Sivaswamy. <b>"A Vessel Keypoint Detector for Junction classification."</b> <em>IEEE International Symposium on Biomedical Imaging <b>(ISBI)</b>, 2017. <b>(Oral)</b></em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950657" target="_blank">Paper</a>]
        [<a href="https://drive.google.com/open?id=1jUuyIlpd-HtjQwWCACoYgK9gnypEH_Oi" target="_blank">Slides</a>]    
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">Code1</a>]--> [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:60px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong><center><u>ABSTRACT</u></center></strong>
		Retinal vessel keypoint detection and classification is a fundamental step in tracking the physiological changes that occur in the retina which is linked to various retinal and systemic diseases. 
		In this paper, we propose a novel Vessel Keypoint Detector (VKD) which is derived from the projection of log-polar transformed binary patches around vessel points. 
		VKD is used to design a two stage solution for junction detection and classification. In the first stage, the keypoints detected using VKD are refined using curvature orientation information to extract candidate junctions. 
		True junctions from these candidates are identified in a supervised manner using a Random Forest classifier. 
		In the next stage, a novel combination of local orientation and shape based features is extracted from the junction points and classified using a second Random Forest classifier. 
		Evaluation results on five datasets show that the designed system is robust to changes in resolution and other variations across datasets, with average values of accuracy/sensitivity/specificity for junction detection being 0.78/0.79/0.75 and for junction classification being 0.87/0.85/0.88. 
		Our system outperforms the state of the art method by at least 11%, on the DRIVE and IOSTAR datasets. 
		These results demonstrate the effectiveness of VKD for vessel analysis.			
</div>
        </td>
    </tr>
    <tr></tr>      <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr>     <tr></tr> <tr></tr> <tr></tr> <tr></tr>  
   
    <tr>
    	<td></td>
        <td>
        <!-- <ul> -->
        <!-- <li> -->
        <p align="justify">
            <b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Recent Advancements in Retinal Vessel Segmentation."</b> <em>Journal of Medical Systems, 2017.</em>
        </p>
        <p>
            [<a href="https://link.springer.com/content/pdf/10.1007%2Fs10916-017-0719-2.pdf" target="_blank">Paper</a>]   [<a href="bibtex.txt">Bibtex</a>] 
        </p>
        <!-- </li> -->
        <!-- </ul> -->
    	<div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
				<strong><center><u>ABSTRACT</u></center></strong>
		<p align="justify">		
		Retinal vessel segmentation is a key step towards the accurate visualization, diagnosis, early treatment and surgery planning of ocular diseases. 
		For the last two decades, a tremendous amount of research has been dedicated in developing automated methods for segmentation of blood vessels from retinal fundus images. 
		Despite the fact, segmentation of retinal vessels still remains a challenging task due to the presence of abnormalities, varying size and shape of the vessels, non-uniform illumination and anatomical variability between subjects. 
		In this paper, we carry out a systematic review of the most recent advancements in retinal vessel segmentation methods published in last five years. The objectives of this study are as follows: 
		first, we discuss the most crucial preprocessing steps that are involved in accurate segmentation of vessels. Second, we review most recent state-of-the-art retinal vessel segmentation techniques which are classified into different categories based on their main principle. 
		Third, we quantitatively analyse these methods in terms of its sensitivity, specificity, accuracy, area under the curve and discuss newly introduced performance metrics in current literature. Fourth, we discuss the advantages and limitations of the existing segmentation techniques. 
		Finally, we provide an insight into active problems and possible future directions towards building successful computer-aided diagnostic system.
</p>
		</div>
	    </td>
    </tr>

</table>


<h2>On-going Projects</h2>
<ul>
    <li>
        <div style="float:left; text-align:left"><b>Automated Classification of Artery/Veins from Retinal Color Fundus Images Using Deep Learning Techniques.</b></div> 
    </li>

    <li>
        <div style="float:left; text-align:left"><b>Automatic Glaucoma Risk Assessment from Multiple Informatic Domains
via Deep Learning Approach.</b></div> 
    </li>
</ul>    


<h2>Experience</h2>
<ul>
	
    <li>
        <div style="float:left; text-align:left">Centre for Visual Information Technology (CVIT), International Institute of Information Technology (IIIT), Hyderabad, India</div> <div style="float:right; text-align:right">Feb. 2019 – Present</div><br>
        <b>Research Associate</b><br>
        Topic: Automatic Glaucoma Assessment from Multiple Informatics Domains via Deep Learning Approach.<br>
    </li>	
	
    <li>
        <div style="float:left; text-align:left">Centre for Visual Information Technology (CVIT), International Institute of Information Technology (IIIT), Hyderabad, India</div> <div style="float:right; text-align:right">Jan. 2016 – Jan. 2017</div><br>
        <b>Research Intern</b><br>
        Topic: A Vessel Keypoint Descriptor for Automatic Detection and Classification of Vessel Junctions.<br>
    </li>
    <li>
        <div style="float:left; text-align:left">Centre for Visual Information Technology (CVIT), International Institute of Information Technology (IIIT), Hyderabad, India</div> <div style="float:right; text-align:right">Feb. 2017 – Dec. 2018</div><br>
        <b>Visiting Doctoral Researcher</b><br>
        Topic: Automatic Segmentation and Quantification of Retinal Vasculature from Color Fundus Images.<br>
    </li>
</ul>


<h2>Awards &amp; Honors</h2>
<table style="border-spacing:2px">
    
        <tbody>
        <tr><td> Shortlisted for Student Paper Competition at 14th International Symposium on Biomedical Imaging (ISBI), Melbourne, Australia, 2018.</td></tr>
        <tr><td> MHRD Fellowship for Ph.D (Govt. of INDIA), 2013.</td></tr>
        
        </tbody>
</table>

<h2>Professional Activities</h2>
<li>
    <b>Membership:</b> IEEE Student, MICCAI Student, EMBC Student, SPS Student.
    <p style="margin-top:3px"></p>
</li>


<div id="footer">
    <div id="footer-text"></div>
</div>
    <p><center>
        
    <br>
        Last updated: 25/02/2019
     
      </center></p>


</div>
</body></html>
