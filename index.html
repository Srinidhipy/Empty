
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Chetan Srinidhi, ECE, NITK, National Institute of Technology Karnataka, IIIT Hyderabad, IIITH"> 
<meta name="description" content="Chetan Srinidhi's home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Chetan Srinidhi, NITK</title>
</head>
<body>
<!-- Initial Photo and Contact Details ***************** -->
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1>Chetan L. Srinidhi</h1><h1>
                </h1></div>

                <h3>Postdoctoral Research Fellow</h3>
                <p>
		    Sunnybrook Research Institute, <br>
                    Medical Biophysics, University of Toronto,<br>
                    Toronto, Canada - M4N3M5.<br>
                    <br>
                </p>
                <p> 
                    <strong>Personal:&nbsp;</strong>&nbsp; [<a href="" target="_blank">CV</a>] / [<a href="https://scholar.google.co.in/citations?user=H6634DcAAAAJ&hl=en" target="_blank">Google Scholar</a>] / [<a href="https://github.com/srinidhiPY" target="_blank">Github</a>] / [<a href="https://twitter.com/ChetanSrinidhi" target="_blank">LinkedIn</a>] <br/><strong>Email:&nbsp;</strong><em>chetan.srinidhi@utoronto.ca / srinidhipy@gmail.com</em> <br/><strong>Research:&nbsp;</strong>&nbsp;[<a href="http://martellab.com/" target="_blank">Martel Lab</a>]
            <td>
                <img src="./images/Chetan2.jpeg" border="0" width="200"><br>
            </td>
        </tr><tr>
    </tr></tbody>
</table>

<!-- Short Bio and Research Interests ***************** -->
<h2>About Me</h2>
<p align="justify">
I am now a Postdoc Fellow at the Sunnybrook Research Institute, Department of Medical Biophysics, University of Toronto, supervised by <a href="https://medbio.utoronto.ca/faculty/martel">Prof. Anne Martel</a>. I work in computational histopathology encompassing machine learning and artificial intelligence based approaches to estimate breast cancer recurrence scores from histopathology whole-slide images.</p>
<p align="justify">
Previously, I completed my Ph.D. at the department of Electronics and Communication Engineering, National Institute of Technology Karnataka (NITK), Surathkal, India, under the supervision of <a href="http://ece.nitk.ac.in/faculty/aparna-p">Dr. Aparna P</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=7YrGeNoAAAAJ&view_op=list_works&sortby=pubdate">Dr. Jeny Rajan</a>. My thesis was focused on "Pattern Recognition and Machine Learning Framework for Automated Analysis of Retinal Images". Before joining NITK for my Ph.D., I obtained my Master's degree from R. V. College of Engineering, Bangalore and Bachelor's degree from Visvesvaraya Technological University, Belgaum, India.  
</p>

<h2>Research Interests</h2>
<p align="justify">
<b>Medical Image Analysis</b>, <b>Computational Pathology</b>, <b>Digital Pathology</b>, <b>Retinal Image Analysis</b>, <b>Machine Learning.</b></p>

<p align="justify">
  My ultimate goal is to bring modern machine learning methods (including deep learning) to biomedical image analysis for improving anatomical structure segmentation, lesion detection and quantification, anatomical landmark identification and computer-aided diagnosis.
  Specifically, I am interested in developing machine learning/deep learning methods for various 2D/3D medical image analysis involving various topics such as: image segmentation, unsupervised representation learning, visual attention modelling, semantic image segmentation, multi-modal learning and domain adaptation. 
  Currently, I am investigating self-supervised and generative adversarial learning related topics in DL for medical scenarios. 
</p>

<!-- Publications ***************** -->
<h2>Publications</h2>
<table style="width:100%">   
    <tr>
        <td width="500">
        <img src="images/Main_review_Figure.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>              
        <td align="justify"><b>Chetan L Srinidhi</b>, Ozan Ciga, Anne L. Martel. <b>"Deep neural network models for computational histopathology: A survey."</b> <em>To appear in arXiV, 2020.</em>
        <p></p>
        <p>[<a href="" target="_blank">Paper</a>]
           [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:150px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
Histopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms
contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a
comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field’s progress based on the methodological
aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning
and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and
highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr>
  	    
    <tr>
        <td width="500">
        <img src="images/TIP_A-V.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Automated Method for Retinal Artery/Vein Separation via Graph Search Metaheuristic Approach."</b> <em>IEEE Transactions on Image Processing <b>(TIP)</b>, 2019. (Top Journal – IF: 6.79)</em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598955" target="_blank">Paper</a>]
        [<a href="https://ieeexplore.ieee.org/ielx7/83/4358840/8598955/Supp_Material_TIP2889534.pdf?tp=&arnumber=8598955" target="_blank">Supplementary Material</a>]    
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]--> [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:50px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		Separation of the vascular tree into arteries and veins is a fundamental prerequisite in the automatic diagnosis of retinal biomarkers associated with systemic and neurodegenerative diseases. 
		In this paper, we present a novel graph search metaheuristic approach for automatic separation of arteries/veins (A/V) from color fundus images. 
		Our method exploits local information to disentangle the complex vascular tree into multiple subtrees, and global information to label these vessel subtrees into arteries and veins. 
		Given a binary vessel map, a graph representation of the vascular network is constructed representing the topological and spatial connectivity of the vascular structures. 
		Based on the anatomical uniqueness at vessel crossing and branching points, the vascular tree is split into multiple subtrees containing arteries and veins. 
		Finally, the identified vessel subtrees are labeled with A/V based on a set of handcrafted features trained with random forest classifier. 
		The proposed method has been tested on four different publicly available retinal datasets with an average accuracy of 94.7%, 93.2%, 96.8% and 90.2% across AV-DRIVE, CT-DRIVE. INSPIRE-AVR and WIDE datasets, respectively. 
		These results demonstrate the superiority of our proposed approach in outperforming state-of-the- art methods for A/V separation.			
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>

	    
     <tr>
        <td width="500">
        <img src="images/BSPC.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"A Visual Attention Guided Unsupervised Feature Learning for Robust Vessel Delineation in Retinal Images."</b> <em>Biomedical Signal Processing and Control, 2018. (IF: 2.943)</em>
        <p></p>
        <p>[<a href="https://www.sciencedirect.com/science/article/pii/S1746809418301010" target="_blank">Paper</a>]
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]-->  [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		We propose a novel visual attention guided unsupervised feature learning (VA-UFL) approach to automatically learn the most discriminative features for segmenting vessels in retinal images. 
		Our VA-UFL approach captures both the knowledge of visual attention mechanism and multi-scale contextual information to selectively visualize the most relevant part of the structure in a given local patch. 
		This allows us to encode a rich hierarchical information into unsupervised filtering learning to generate a set of most discriminative features that aid in the accurate segmentation of vessels, even in the presence of cluttered background.
		Our proposed method is validated on the five publicly available retinal datasets: DRIVE, STARE, CHASE_DB1, IOSTAR and RC-SLO. 
		The experimental results show that the proposed approach significantly outperformed the state-of-the-art methods in terms of sensitivity, accuracy and area under the receiver operating characteristic curve across all five datasets. 
		Specifically, the method achieved an average sensitivity greater than 0.82, which is 7% higher compared to all existing approaches validated on DRIVE, CHASE_DB1, IOSTAR and RC-SLO datasets, and outperformed even second-human observer. 
		The method is shown to be robust to segmentation of thin vessels, strong central vessel reflex, complex crossover structures and fares well on abnormal cases.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>  

	    
    <tr>
        <td width="500">
        <img src="images/ISBI17.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, Priyadarshi Rath, Jayanthi Sivaswamy. <b>"A Vessel Keypoint Detector for Junction classification."</b> <em>IEEE International Symposium on Biomedical Imaging <b>(ISBI)</b>, 2017. <b>(Oral Presentation, Acceptance Rate: ~19%)</b></em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950657" target="_blank">Paper</a>]
        [<a href="https://drive.google.com/open?id=1jUuyIlpd-HtjQwWCACoYgK9gnypEH_Oi" target="_blank">Slides</a>]    
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">Code1</a>]--> [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:60px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		Retinal vessel keypoint detection and classification is a fundamental step in tracking the physiological changes that occur in the retina which is linked to various retinal and systemic diseases. 
		In this paper, we propose a novel Vessel Keypoint Detector (VKD) which is derived from the projection of log-polar transformed binary patches around vessel points. 
		VKD is used to design a two stage solution for junction detection and classification. In the first stage, the keypoints detected using VKD are refined using curvature orientation information to extract candidate junctions. 
		True junctions from these candidates are identified in a supervised manner using a Random Forest classifier. 
		In the next stage, a novel combination of local orientation and shape based features is extracted from the junction points and classified using a second Random Forest classifier. 
		Evaluation results on five datasets show that the designed system is robust to changes in resolution and other variations across datasets, with average values of accuracy/sensitivity/specificity for junction detection being 0.78/0.79/0.75 and for junction classification being 0.87/0.85/0.88. 
		Our system outperforms the state of the art method by at least 11%, on the DRIVE and IOSTAR datasets. 
		These results demonstrate the effectiveness of VKD for vessel analysis.			
</div>
        </td>
    </tr>
    <tr></tr>      <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr>     <tr></tr> <tr></tr> <tr></tr> <tr></tr>  
   
    <tr>
    	<td></td>
        <td>
        <!-- <ul> -->
        <!-- <li> -->
        <p align="justify">
            <b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Recent Advancements in Retinal Vessel Segmentation."</b> <em>Journal of Medical Systems, 2017. <font color="#A52A2A">(IF: 2.415)</font></em>
        </p>
        <p>
            [<a href="https://link.springer.com/content/pdf/10.1007%2Fs10916-017-0719-2.pdf" target="_blank">Paper</a>]   [<a href="bibtex.txt">Bibtex</a>] 
        </p>
        <!-- </li> -->
        <!-- </ul> -->
    	<div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		<p align="justify">		
		Retinal vessel segmentation is a key step towards the accurate visualization, diagnosis, early treatment and surgery planning of ocular diseases. 
		For the last two decades, a tremendous amount of research has been dedicated in developing automated methods for segmentation of blood vessels from retinal fundus images. 
		Despite the fact, segmentation of retinal vessels still remains a challenging task due to the presence of abnormalities, varying size and shape of the vessels, non-uniform illumination and anatomical variability between subjects. 
		In this paper, we carry out a systematic review of the most recent advancements in retinal vessel segmentation methods published in last five years. The objectives of this study are as follows: 
		first, we discuss the most crucial preprocessing steps that are involved in accurate segmentation of vessels. Second, we review most recent state-of-the-art retinal vessel segmentation techniques which are classified into different categories based on their main principle. 
		Third, we quantitatively analyse these methods in terms of its sensitivity, specificity, accuracy, area under the curve and discuss newly introduced performance metrics in current literature. Fourth, we discuss the advantages and limitations of the existing segmentation techniques. 
		Finally, we provide an insight into active problems and possible future directions towards building successful computer-aided diagnostic system.
</p>
		</div>
	    </td>
    </tr>

</table>

<h2>Professional Activities</h2>
<li>
    <b>Reviewer for:</b> IEEE Journal of Biomedical and Health Informatics (JBHI), IEEE Transactions on Industrial Informatics (TII), IEEE Access.
    <p style="margin-top:3px"></p>
</li>


<div id="footer">
    <div id="footer-text"></div>
</div>
    <p><center>
        
    <br>
        Last updated: 02/Jan/2020
     
      </center></p>


</div>
</body></html>
